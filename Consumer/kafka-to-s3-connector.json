{
    "name": "kafka-to-s3",
    "config": {
      "_comment": "The S3 sink connector class",
      "connector.class": "io.confluent.connect.s3.S3SinkConnector",
      "_comment": "The total number of Connect tasks to spawn (with implicit upper limit the number of topic-partitions)",
      "tasks.max": "1",
      "_comment": "Which topics to export to S3",
      "topics": "dbserver1.public.customers",
      "_comment": "The S3 bucket that will be used by this connector instance",
      "s3.bucket.name": "dataorc-in-raw-data",
      "_comment": "The AWS region where the S3 bucket is located",
      "s3.region": "ap-south-1",
      "_comment": "The size in bytes of a single part in a multipart upload. The last part is of s3.part.size bytes or less. This property does not affect the total size of an S3 object uploaded by the S3 connector",
      "s3.part.size": "5242880",
      "_comment": "The maximum number of Kafka records contained in a single S3 object. Here a high value to allow for time-based partition to take precedence",
      "flush.size": "5000000",
      "_comment": "Kafka Connect converter used to deserialize keys (unused in this example)",
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "key.converter.schemas.enable": "false",
      "_comment": "Kafka Connect converter used to deserialize values",
      "value.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter.schemas.enable": "false",
      "_comment": "The type of storage for this storage cloud connector",
      "storage.class": "io.confluent.connect.s3.storage.S3Storage",
      "_comment": "The storage format of the objects uploaded to S3",
      "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
      "_comment": "Schema compatibility mode between records with schemas (Useful when used with schema-based converters. Unused in this example, listed for completeness)",
      "schema.compatibility": "NONE",
      "_comment": "The class used to partition records in objects to S3. Here, partitioning based on time is used.",
      "partitioner.class": "io.confluent.connect.storage.partitioner.TimeBasedPartitioner",
      "_comment": "The locale used by the time-based partitioner to encode the date string",
      "locale": "en",
      "_comment": "Setting the timezone of the timestamps is also required by the time-based partitioner",
      "timezone": "UTC",
      "_comment": "The date-based part of the S3 object key",
      "path.format": "'server_date'=YYYY-MM-dd/'hour'=HH/'minute'=mm",
      "_comment": "The duration that aligns with the path format defined above",
      "partition.duration.ms": "1800000",
      "_comment": "The interval between timestamps that is sufficient to upload a new object to S3. Here a small interval of 1min for better visualization during the demo",
      "rotate.interval.ms": "-1",
      "rotate.schedule.interval.ms": "1800000",
      "_comment": "The class to use to derive the timestamp for each record. Here Kafka record timestamps are used",
      "timestamp.extractor": "Wallclock"
    }
  }